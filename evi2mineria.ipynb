{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Error loading wordnet: <urlopen error [SSL:\n",
      "[nltk_data]     CERTIFICATE_VERIFY_FAILED] certificate verify failed:\n",
      "[nltk_data]     unable to get local issuer certificate (_ssl.c:997)>\n",
      "[nltk_data] Error loading omw-1.4: <urlopen error [SSL:\n",
      "[nltk_data]     CERTIFICATE_VERIFY_FAILED] certificate verify failed:\n",
      "[nltk_data]     unable to get local issuer certificate (_ssl.c:997)>\n",
      "[nltk_data] Error loading punkt: <urlopen error [SSL:\n",
      "[nltk_data]     CERTIFICATE_VERIFY_FAILED] certificate verify failed:\n",
      "[nltk_data]     unable to get local issuer certificate (_ssl.c:997)>\n",
      "[nltk_data] Error loading averaged_perceptron_tagger: <urlopen error\n",
      "[nltk_data]     [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify\n",
      "[nltk_data]     failed: unable to get local issuer certificate\n",
      "[nltk_data]     (_ssl.c:997)>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import nltk\n",
    "import unidecode\n",
    "\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from nltk.stem import PorterStemmer\n",
    "from textblob import TextBlob\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "from wordcloud import WordCloud\n",
    "from PIL import Image\n",
    "\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "nltk.download(\"punkt\")\n",
    "nltk.download(\"averaged_perceptron_tagger\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /Users/gaby/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import ssl\n",
    "try:\n",
    "     _create_unverified_https_context =     ssl._create_unverified_context\n",
    "except AttributeError:\n",
    "     pass\n",
    "else:\n",
    "    ssl._create_default_https_context = _create_unverified_https_context\n",
    "\n",
    "nltk.download('stopwords')\n",
    "stop_words_sp = set(stopwords.words('spanish'))\n",
    "stop_words_en = set(stopwords.words('english'))\n",
    "stop_words = stop_words_sp | stop_words_en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tweets = pd.read_csv('turismo.csv', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tweets['content']=df_tweets['content'].str.normalize('NFKD').str.encode('ascii', errors='ignore').str.decode('utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_rt(x): return re.sub('RT @\\w+: ','', x)\n",
    "def rt(x): return re.sub('(@[A-Za-z0â€“9]+)|[^a-zA-Z]', ' ', x)\n",
    "\n",
    "\n",
    "df_tweets['content'] = df_tweets['content'].map(rt).map(remove_rt)\n",
    "df_tweets['content'] = df_tweets['content'].str.lower()\n",
    "X = df_tweets['content']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_data = []\n",
    "manually_excluded = ['co', 'amp', 'https','http']\n",
    "for i in range(len(X)):\n",
    "    tweet = re.sub('[^a-zA-Z]', ' ', X.iloc[i])\n",
    "    tweet = tweet.lower().split()\n",
    "    tweet = [word for word in tweet if (\n",
    "        word not in stop_words) and (word not in manually_excluded)]\n",
    "    tweet = [word for word in tweet if (wordnet.synsets(word))]\n",
    "    tweet = ' '.join(tweet)\n",
    "    cleaned_data.append(tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tweets['text'] = cleaned_data\n",
    "positive = 0\n",
    "negative = 0\n",
    "neutral = 0\n",
    "polarity = 0\n",
    "tweet_list = []\n",
    "neutral_list = []\n",
    "negative_list = []\n",
    "positive_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def percentage(part, whole):\n",
    "    return 100 * float(part)/float(whole)\n",
    "\n",
    "\n",
    "noOfTweet = len(df_tweets)\n",
    "sentiment_analyzer = SentimentIntensityAnalyzer()\n",
    "for index, tweet in df_tweets.iterrows():\n",
    "    tweet_list.append(tweet['text'])\n",
    "    analysis = TextBlob(tweet['text'])\n",
    "    score = sentiment_analyzer.polarity_scores(tweet['text'])\n",
    "    neg = score['neg']\n",
    "    neu = score['neu']\n",
    "    pos = score['pos']\n",
    "    comp = score['compound']\n",
    "    polarity += analysis.sentiment.polarity\n",
    "\n",
    "    if neg > pos:\n",
    "        negative_list.append(tweet['text'])\n",
    "    elif pos > neg:\n",
    "        positive_list.append(tweet['text'])\n",
    "    elif pos == neg:\n",
    "        neutral_list.append(tweet['text'])\n",
    "\n",
    "    positive = percentage(len(positive_list), noOfTweet)\n",
    "    negative = percentage(len(negative_list), noOfTweet)\n",
    "    neutral = percentage(len(neutral_list), noOfTweet)\n",
    "    polarity = percentage(polarity, noOfTweet)\n",
    "    positive = format(positive, '.1f')\n",
    "    negative = format(negative, '.1f')\n",
    "    neutral = format(neutral, '.1f')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = ['Positive ['+str(positive)+'%]', 'Neutral [' +\n",
    "          str(neutral)+'%]', 'Negative ['+str(negative)+'%]']\n",
    "sizes = [positive, neutral, negative]\n",
    "colors = ['yellowgreen', 'blue', 'red']\n",
    "patches, texts = plt.pie(sizes, colors=colors, startangle=90)\n",
    "plt.style.use('default')\n",
    "plt.legend(labels)\n",
    "plt.title(\"Sentiment Analysis Result\")\n",
    "plt.axis('equal')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_wordcloud(text):\n",
    "    wc = WordCloud(background_color='white', max_words=3000)\n",
    "    wc.generate(str(text))\n",
    "    plt.imshow(wc, interpolation='bilinear')\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "create_wordcloud(pd.Series(positive_list).values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_wordcloud(text):\n",
    "    wc = WordCloud(background_color='white', max_words=3000)\n",
    "    wc.generate(str(text))\n",
    "    plt.imshow(wc, interpolation='bilinear')\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "create_wordcloud(pd.Series(negative_list).values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_wordcloud(text):\n",
    "    wc = WordCloud(background_color='white', max_words=3000)\n",
    "    wc.generate(str(text))\n",
    "    plt.imshow(wc, interpolation='bilinear')\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "create_wordcloud(pd.Series(tweet_list).values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_wordcloud(text):\n",
    "    wc = WordCloud(background_color='white', max_words=3000)\n",
    "    wc.generate(str(text))\n",
    "    plt.imshow(wc, interpolation='bilinear')\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "create_wordcloud(pd.Series(neutral_list).values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(df_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "contenido=df_tweets[\"content\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "contenido.to_csv(\"contenidotweets.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
